{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwx------   - stud12 stud12          0 2021-06-12 11:08 /user/stud12/.Trash\n",
      "drwxr-xr-x   - stud12 stud12          0 2021-06-10 21:57 /user/stud12/.sparkStaging\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /user/stud12/ #Presenting the folders that exsist in the HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -mkdir /user/stud12/test ##Creating a folder for our data for this assignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -put /home/stud12/Desktop/microsoft-com.data/ /user/stud12/test/ ##Uploaing the files to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   3 stud12 stud12        387 2021-06-12 11:09 /user/stud12/test/microsoft-com.data/countries.txt\n",
      "-rw-r--r--   3 stud12 stud12    1491520 2021-06-12 11:09 /user/stud12/test/microsoft-com.data/microsoft-com.data\n",
      "-rw-r--r--   3 stud12 stud12       3629 2021-06-12 11:09 /user/stud12/test/microsoft-com.data/microsoft-com.info\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /user/stud12/test/microsoft-com.data #showing the current files in the HDFS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Converting data to RDDs \n",
    "myrdd = sc.textFile(\"hdfs:/user/stud12/test/microsoft-com.data/microsoft-com.data\")\n",
    "\n",
    "countries_rdd=sc.textFile(\"hdfs:/user/stud12/test/microsoft-com.data/countries.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Brazil', u'Canada', u'Caribbean', u'Czech Republic', u'France', u'Slovakia', u'Ireland', u'Italy', u'Argentina', u'Norway', u'Israel', u'Australia', u'Turkey', u'Venezuela', u'China', u'Chile', u'Jakarta', u'Belgium', u'Germany', u'Hong Kong', u'Spain', u'Netherlands', u'UK', u'Denmark', u'Poland', u'Finland', u'Sweden', u'Korea', u'Thailand', u'Switzerland', u'Uruguay', u'New Zealand', u'Russia', u'Portugal', u'Mexico', u'South Africa', u'India', u'Peru', u'Colombia', u'Hungary', u'Taiwan', u'Slovenija']\n"
     ]
    }
   ],
   "source": [
    "# Get list of unique countris from the data  \n",
    "countries_list = list(set(countries_rdd.collect())) \n",
    "print countries_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Attributes data - filtering the data and keeping only the rows with the relevant countries \n",
    "## input: original RDD\n",
    "##output: RDD with relevant countries ([attribute ID, country,url])\n",
    "\n",
    "Attributes=myrdd.filter(lambda line: len(line.split()) >0)\\\n",
    ".map(lambda line: line.split(','))\\\n",
    ".filter(lambda line: 'A' in line[0])\\\n",
    ".map(lambda line: [str(line[1]),str(line[3]).replace('\"',''),str(line[4]).replace('\"','')])\\\n",
    ".filter(lambda line: line[1] in countries_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Vote data -  filtering the relevant rows \n",
    "##input: original data\n",
    "##output: [attribue ID,userID]. we changed the order in order to join attributes with votes \n",
    "votes=myrdd.filter(lambda line: len(line.split()) >0)\\\n",
    ".map(lambda line: line.split(','))\\\n",
    ".filter(lambda line: 'V' in line[0])\\\n",
    ".map(lambda line: [str(line[2]),str(line[1])])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1142', ('South Africa', '10372')), ('1142', ('South Africa', '13352'))]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join attributes with Votes, the key is attribue ID (=page number)\n",
    "#output: [attribute ID, (country,userID)]\n",
    "countries_users = Attributes.join(votes)\n",
    "countries_users.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|       Country|CountUser|\n",
      "+--------------+---------+\n",
      "|        Brazil|      121|\n",
      "|        Canada|      128|\n",
      "|         Italy|      167|\n",
      "|     Hong Kong|       35|\n",
      "|        Turkey|        9|\n",
      "|         India|        9|\n",
      "|     Venezuela|        8|\n",
      "|   Switzerland|       31|\n",
      "|         China|       26|\n",
      "|      Slovakia|       11|\n",
      "|       Belgium|       45|\n",
      "|     Caribbean|        5|\n",
      "|       Germany|      372|\n",
      "|        Norway|       42|\n",
      "|Czech Republic|       16|\n",
      "|       Denmark|       55|\n",
      "|       Jakarta|      670|\n",
      "|       Hungary|       15|\n",
      "|        Russia|       52|\n",
      "|      Thailand|       11|\n",
      "+--------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "[('Jakarta', 670), ('Germany', 372), ('Sweden', 258), ('Taiwan', 204), ('Spain', 191)]\n"
     ]
    }
   ],
   "source": [
    "##question4\n",
    "##filtering the unique lines and creating map reduce of [country,number of users per country] \n",
    "number_of_users_per_country= countries_users.map(lambda line: (line[0],line[1])).distinct()\\\n",
    ".map(lambda line: (line[1][0],1))\\\n",
    " .reduceByKey(lambda v1, v2: v1+v2)\n",
    "    \n",
    "##Convering to dataframe for visualization     \n",
    "report1=number_of_users_per_country.toDF(['Country','CountUser'])\n",
    "report1.show()\n",
    "\n",
    "##Exctracting the top 5 results \n",
    "top_5 = number_of_users_per_country.top(5, key=lambda x: x[1])\n",
    "print top_5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Saving the file in HDFS\n",
    "number_of_users_per_country.saveAsTextFile(\"hdfs:/user/stud12/test/number_of_users_per_country\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Brazil', 121)\n",
      "('Canada', 128)\n",
      "('Italy', 167)\n",
      "('Hong Kong', 35)\n",
      "('Turkey', 9)\n",
      "('India', 9)\n",
      "('Venezuela', 8)\n",
      "('Switzerland', 31)\n",
      "('China', 26)\n",
      "('Slovakia', 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -cat /user/stud12/test/number_of_users_per_country/part-* | head ##showing the file was saved in HDFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1142', ('South Africa', '10372')),\n",
       " ('1142', ('South Africa', '13352')),\n",
       " ('1142', ('South Africa', '19019'))]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#question #6\n",
    "#1. orgnizinf the data\n",
    "## Attributes data - filtering and orgnizing the data \n",
    "## input: original RDD\n",
    "##output: filtered RDD ([attribute ID, country,url])\n",
    "\n",
    "pages=myrdd.filter(lambda line: len(line.split()) >0)\\\n",
    ".map(lambda line: line.split(','))\\\n",
    ".filter(lambda line: 'A' in line[0])\\\n",
    ".map(lambda line: [str(line[1]),str(line[3]).replace('\"',''),str(line[4]).replace('\"','')])\n",
    "\n",
    "#2. join with votes data\n",
    "\n",
    "pages_users=pages.join(votes)\n",
    "pages_users.take(3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average number is 3.01592736388\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##finding the average number of pages per user \n",
    "\n",
    "##creating list of unique users\n",
    "users_rdd=votes.map(lambda line: line[1])\n",
    "unique_users=list(set(users_rdd.collect()))\n",
    "\n",
    "\n",
    "##creating tuple of[user,number of pages] using map reduce\n",
    "count_pages_per_user=pages_users.map(lambda line: (line[0],line[1][1])).distinct()\\\n",
    "                .map(lambda line:(line[1],1))\\\n",
    " .reduceByKey(lambda v1,v2:(v1+v2))\n",
    "\n",
    "##Total number of pages \n",
    "total_pages=count_pages_per_user.map(lambda line: (1,line[1]))\\\n",
    ".reduceByKey(lambda v1,v2:(v1+v2))\n",
    "\n",
    "##calculating the average per user \n",
    "average=1.0*total_pages.take(1)[0][1]/len(unique_users)\n",
    "print \"the average number is \" + str(float(average))\n",
    "\n",
    "\n",
    "                         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The page id with the maximum number of visits is: 1008 ,the number of visits: 10836\n"
     ]
    }
   ],
   "source": [
    "#Question #7\n",
    "##creating tuple of[page,count_visits] and finding the page ID with maximum number of visit \n",
    "results=pages_users.map(lambda line: (line[0],line[1][1])).distinct()\\\n",
    "                .map(lambda line:(line[0],1))\\\n",
    " .reduceByKey(lambda v1,v2:(v1+v2)).top(1, key=lambda x: x[1])\n",
    "\n",
    "print \"The page id with the maximum number of visits is: {} ,\\\n",
    "the number of visits: {}\".format(results[0][0], results[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `/user/stud12/test': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "hdfs dfs -rm -r /user/stud12/test ### Deleting our files from HDFS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark2(2.1.0)",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
